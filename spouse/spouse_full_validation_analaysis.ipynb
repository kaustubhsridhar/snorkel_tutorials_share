{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting spouse mentions in sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "In this tutorial, we will see how Snorkel can be used for Information Extraction. We will walk through an example text classification task for information extraction, where we use labeling functions involving keywords and distant supervision.\n",
    "### Classification Task\n",
    "<img src=\"imgs/sentence.jpg\" width=\"700px;\" onerror=\"this.onerror=null; this.src='/doks-theme/assets/images/sentence.jpg';\" align=\"center\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "We want to classify each __candidate__ or pair of people mentioned in a sentence, as being married at some point or not.\n",
    "\n",
    "In the above example, our candidate represents the possible relation `(Barack Obama, Michelle Obama)`. As readers, we know this mention is true due to external knowledge and the keyword of `wedding` occuring later in the sentence.\n",
    "We begin with some basic setup and data downloading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "md-exclude"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"snorkel-tutorials\":\n",
    "    os.chdir(\"spouse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "\n",
    "((df_dev, Y_dev), df_train, (df_test, Y_test)) = load_data()\n",
    "print(df_dev.shape, df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input Data:** `df_dev`, `df_train`, and `df_test` are `Pandas DataFrame` objects, where each row represents a particular __candidate__. For our problem, a candidate consists of a sentence, and two people mentioned in the sentence. The DataFrames contain the fields `sentence`, which refers to the sentence of the candidate, `tokens`, the tokenized form of the sentence, and `person1_word_idx` and `person2_word_idx`, which represent `[start, end]` indices in the tokens at which the first and second person's name appear, respectively.\n",
    "\n",
    "We also have certain **preprocessed fields**, that we discuss a few cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "md-exclude"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# Don't truncate text fields in the display\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a candidate in the development set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from preprocessors import get_person_text\n",
    "\n",
    "candidate = df_dev.loc[2]\n",
    "person_names = get_person_text(candidate).person_names\n",
    "\n",
    "print(\"Sentence: \", candidate[\"sentence\"])\n",
    "print(\"Person 1: \", person_names[0])\n",
    "print(\"Person 2: \", person_names[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data\n",
    "\n",
    "In a real application, there is a lot of data preparation, parsing, and database loading that needs to be completed before we generate candidates and dive into writing labeling functions. Here we've pre-generated candidates in a pandas DataFrame object per split (train,dev,test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Function Helpers\n",
    "\n",
    "When writing labeling functions, there are several functions you will use over and over again. In the case of text relation extraction as with this task, common functions include those for fetching text between mentions of the two people in a candidate, examing word windows around person mentions, and so on. We will wrap these functions as `preprocessors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from snorkel.preprocess import preprocessor\n",
    "\n",
    "\n",
    "@preprocessor()\n",
    "def get_text_between(cand):\n",
    "    \"\"\"\n",
    "    Returns the text between the two person mentions in the sentence for a candidate\n",
    "    \"\"\"\n",
    "    start = cand.person1_word_idx[1] + 1\n",
    "    end = cand.person2_word_idx[0]\n",
    "    cand.text_between = \" \".join(cand.tokens[start:end])\n",
    "    return cand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate PreProcessors\n",
    "\n",
    "For the purposes of the tutorial, we have three fields (`between_tokens`, `person1_right_tokens`, `person2_right_tokens`) preprocessed in the data, which can be used when creating labeling functions. We also provide the following set of `preprocessor`s for this task in `preprocessors.py`, along with the fields these populate.\n",
    "* `get_person_text(cand)`: `person_names`\n",
    "* `get_person_lastnames(cand)`: `person_lastnames`\n",
    "* `get_left_tokens(cand)`: `person1_left_tokens`, `person2_left_tokens`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from preprocessors import get_left_tokens, get_person_last_names\n",
    "\n",
    "POSITIVE = 1\n",
    "NEGATIVE = 0\n",
    "ABSTAIN = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "# Check for the `spouse` words appearing between the person mentions\n",
    "spouses = {\"spouse\", \"wife\", \"husband\", \"ex-wife\", \"ex-husband\"}\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(spouses=spouses))\n",
    "def lf_husband_wife(x, spouses):\n",
    "    return POSITIVE if len(spouses.intersection(set(x.between_tokens))) > 0 else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check for the `spouse` words appearing to the left of the person mentions\n",
    "@labeling_function(resources=dict(spouses=spouses), pre=[get_left_tokens])\n",
    "def lf_husband_wife_left_window(x, spouses):\n",
    "    if len(set(spouses).intersection(set(x.person1_left_tokens))) > 0:\n",
    "        return POSITIVE\n",
    "    elif len(set(spouses).intersection(set(x.person2_left_tokens))) > 0:\n",
    "        return POSITIVE\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check for the person mentions having the same last name\n",
    "@labeling_function(pre=[get_person_last_names])\n",
    "def lf_same_last_name(x):\n",
    "    p1_ln, p2_ln = x.person_lastnames\n",
    "\n",
    "    if p1_ln and p2_ln and p1_ln == p2_ln:\n",
    "        return POSITIVE\n",
    "    return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check for the word `married` between person mentions\n",
    "@labeling_function()\n",
    "def lf_married(x):\n",
    "    return POSITIVE if \"married\" in x.between_tokens else ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check for words that refer to `family` relationships between and to the left of the person mentions\n",
    "family = {\n",
    "    \"father\",\n",
    "    \"mother\",\n",
    "    \"sister\",\n",
    "    \"brother\",\n",
    "    \"son\",\n",
    "    \"daughter\",\n",
    "    \"grandfather\",\n",
    "    \"grandmother\",\n",
    "    \"uncle\",\n",
    "    \"aunt\",\n",
    "    \"cousin\",\n",
    "}\n",
    "family = family.union({f + \"-in-law\" for f in family})\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(family=family))\n",
    "def lf_familial_relationship(x, family):\n",
    "    return NEGATIVE if len(family.intersection(set(x.between_tokens))) > 0 else ABSTAIN\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(family=family), pre=[get_left_tokens])\n",
    "def lf_family_left_window(x, family):\n",
    "    if len(set(family).intersection(set(x.person1_left_tokens))) > 0:\n",
    "        return NEGATIVE\n",
    "    elif len(set(family).intersection(set(x.person2_left_tokens))) > 0:\n",
    "        return NEGATIVE\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check for `other` relationship words between person mentions\n",
    "other = {\"boyfriend\", \"girlfriend\", \"boss\", \"employee\", \"secretary\", \"co-worker\"}\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(other=other))\n",
    "def lf_other_relationship(x, other):\n",
    "    return NEGATIVE if len(other.intersection(set(x.between_tokens))) > 0 else ABSTAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distant Supervision Labeling Functions\n",
    "\n",
    "In addition to using factories that encode pattern matching heuristics, we can also write labeling functions that _distantly supervise_ data points. Here, we'll load in a list of known spouse pairs and check to see if the pair of persons in a candidate matches one of these.\n",
    "\n",
    "[**DBpedia**](http://wiki.dbpedia.org/): Our database of known spouses comes from DBpedia, which is a community-driven resource similar to Wikipedia but for curating structured data. We'll use a preprocessed snapshot as our knowledge base for all labeling function development.\n",
    "\n",
    "We can look at some of the example entries from DBPedia and use them in a simple distant supervision labeling function.\n",
    "\n",
    "Make sure `dbpedia.pkl` is in the `spouse/data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(\"data/dbpedia.pkl\", \"rb\") as f:\n",
    "    known_spouses = pickle.load(f)\n",
    "\n",
    "list(known_spouses)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@labeling_function(resources=dict(known_spouses=known_spouses), pre=[get_person_text])\n",
    "def lf_distant_supervision(x, known_spouses):\n",
    "    p1, p2 = x.person_names\n",
    "    if (p1, p2) in known_spouses or (p2, p1) in known_spouses:\n",
    "        return POSITIVE\n",
    "    else:\n",
    "        return ABSTAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from preprocessors import last_name\n",
    "\n",
    "# Last name pairs for known spouses\n",
    "last_names = set(\n",
    "    [\n",
    "        (last_name(x), last_name(y))\n",
    "        for x, y in known_spouses\n",
    "        if last_name(x) and last_name(y)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "@labeling_function(resources=dict(last_names=last_names), pre=[get_person_last_names])\n",
    "def lf_distant_supervision_last_names(x, last_names):\n",
    "    p1_ln, p2_ln = x.person_lastnames\n",
    "\n",
    "    return (\n",
    "        POSITIVE\n",
    "        if (p1_ln != p2_ln)\n",
    "        and ((p1_ln, p2_ln) in last_names or (p2_ln, p1_ln) in last_names)\n",
    "        else ABSTAIN\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Labeling Functions to the Data\n",
    "We create a list of labeling functions and apply them to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "lfs = [\n",
    "    lf_husband_wife,\n",
    "    lf_husband_wife_left_window,\n",
    "    lf_same_last_name,\n",
    "    lf_married,\n",
    "    lf_familial_relationship,\n",
    "    lf_family_left_window,\n",
    "    lf_other_relationship,\n",
    "    lf_distant_supervision,\n",
    "    lf_distant_supervision_last_names,\n",
    "]\n",
    "applier = PandasLFApplier(lfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from snorkel.labeling import LFAnalysis\n",
    "\n",
    "L_dev = applier.apply(df_dev)\n",
    "L_train = applier.apply(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LFAnalysis(L_dev, lfs).lf_summary(Y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports for dependencies and informed snorkel label_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from Our_Monitors.CD_Monitor import CDM, Informed_LabelModel\n",
    "from Our_Monitors.CDGA_Monitor import CDGAM\n",
    "from Our_Monitors.New_Monitor import NM\n",
    "from Our_Monitors.utils import ModVarma_InCov\n",
    "\n",
    "from dependency_model.varma_deps_functions import get_varma_edges, get_varma_with_gold_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toggle all warnings js code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "(function(on) {\n",
    "const e=$( \"<a>Setup failed</a>\" );\n",
    "const ns=\"js_jupyter_suppress_warnings\";\n",
    "var cssrules=$(\"#\"+ns);\n",
    "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
    "e.click(function() {\n",
    "    var s='Showing';  \n",
    "    cssrules.empty()\n",
    "    if(on) {\n",
    "        s='Hiding';\n",
    "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
    "    }\n",
    "    e.text(s+' warnings (click to toggle)');\n",
    "    on=!on;\n",
    "}).click();\n",
    "$(element).append(e);\n",
    "})(true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_test = applier.apply(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "epochs_list = [50, 100, 500, 1000, 5000][::-1]\n",
    "sig_list = [0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "sig_crazy_list = [0.75, 0.9]\n",
    "thresh_list = [0.1, 0.5, 1, 1.5]\n",
    "lr_list = [0.01, 0.025, 0.05, 0.1]\n",
    "\n",
    "deps_names = ['NM', 'CDGAM', 'Varma_Gold', 'Varma', 'Empty'] # !!!!!!!!!! modify this !!!!!!!!!!!\n",
    "\n",
    "info = { 'CDM': {'deps_params': sig_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'NM': {'deps_params': sig_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'NM_NP': {'deps_params': sig_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'CDGAM': {'deps_params': sig_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'Mod_Varma': {'deps_params': thresh_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'Varma': {'deps_params': thresh_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'Varma_Gold': {'deps_params': thresh_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'Empty': {'deps_params': [-1], 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}}\n",
    "\n",
    "roc_based_store = {key: {'param': -1, 'n_eps': -1, 'lr': -1, 'f1': -1, 'roc_auc': -1} for key in deps_names}\n",
    "\n",
    "f1_based_store = {key: {'param': -1, 'n_eps': -1, 'lr': -1, 'f1': -1, 'roc_auc': -1} for key in deps_names}\n",
    "\n",
    "def overall_deps_fn(deps_name, param):\n",
    "    if deps_name == 'CDM':\n",
    "        deps = CDM(L_dev, Y_dev, k=2, sig=param, policy = 'old', verbose=False, return_more_info = False)\n",
    "    elif deps_name == 'CDGAM':\n",
    "        deps = CDGAM(L_dev, k=2, sig=param, policy = 'old', verbose = False, return_more_info = False)\n",
    "    elif deps_name == 'NM':\n",
    "        deps = NM(L_dev, Y_dev, k=2, sig=param, policy = 'old', verbose=False, return_more_info = False)\n",
    "    elif deps_name == 'NM_NP':\n",
    "        deps = NM(L_dev, Y_dev, k=2, sig=param, policy = 'new', verbose=False, return_more_info = False)\n",
    "    elif deps_name == 'Mod_Varma':\n",
    "        deps = ModVarma_InCov(L_dev, Y_dev, thresh=param)\n",
    "    elif deps_name == 'Varma':\n",
    "        deps = get_varma_edges(L_dev, thresh=param)\n",
    "    elif deps_name == 'Varma_Gold':\n",
    "        deps = get_varma_with_gold_edges(L_dev, Y_dev, thresh=param)\n",
    "    elif deps_name == 'Empty':\n",
    "        deps = []\n",
    "    return deps\n",
    "\n",
    "ct=0\n",
    "total = sum([len(info[deps_name]['deps_params']) for deps_name in deps_names]) * len(epochs_list) * len(lr_list)\n",
    "for deps_name in deps_names:\n",
    "    for param in info[deps_name]['deps_params']:\n",
    "        deps = overall_deps_fn(deps_name, param)\n",
    "        \n",
    "        for n_eps in info[deps_name]['snorkel_eps']:\n",
    "            for lr in info[deps_name]['snorkel_lr']:\n",
    "                \n",
    "                label_model = Informed_LabelModel(edges = deps, cardinality=2, verbose=True)\n",
    "                label_model.fit(L_train, Y_dev, seed=12345, lr=lr, log_freq=n_eps/10, n_epochs=n_eps)\n",
    "                \n",
    "                probs_dev = label_model.predict_proba(L_dev)\n",
    "                preds_dev = probs_to_preds(probs_dev)\n",
    "                f1 = metric_score(Y_dev, preds_dev, probs=probs_dev, metric='f1')\n",
    "                roc_auc = metric_score(Y_dev, preds_dev, probs=probs_dev, metric='roc_auc')\n",
    "                \n",
    "                if roc_auc>roc_based_store[deps_name]['roc_auc']:\n",
    "                    print(deps_name, param, deps, n_eps, lr, \" | roc: \", roc_auc)\n",
    "                    roc_based_store[deps_name] = {'param': param, 'n_eps': n_eps, 'lr': lr, 'f1': f1, 'roc_auc': roc_auc}\n",
    "                \n",
    "                if f1>f1_based_store[deps_name]['f1']:\n",
    "                    print(deps_name, param, deps, n_eps, lr, \" | f1: \", f1)\n",
    "                    f1_based_store[deps_name] = {'param': param, 'n_eps': n_eps, 'lr': lr, 'f1': f1, 'roc_auc': roc_auc}   \n",
    "                \n",
    "                ct +=1\n",
    "                print(ct, \" / \", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# test using params from validation\n",
    "f1_based_test_store = {key: {'f1': -1, 'roc_auc': -1} for key in deps_names}\n",
    "roc_based_test_store = {key: {'f1': -1, 'roc_auc': -1} for key in deps_names}\n",
    "\n",
    "for deps_name in deps_names:\n",
    "    param = f1_based_store[deps_name]['param']\n",
    "    n_eps = f1_based_store[deps_name]['n_eps']\n",
    "    lr = f1_based_store[deps_name]['lr']\n",
    "    \n",
    "    deps = overall_deps_fn(deps_name, param)\n",
    "    \n",
    "    label_model = Informed_LabelModel(edges = deps, cardinality=2, verbose=True)\n",
    "    label_model.fit(L_train, Y_dev, seed=12345, lr=lr, log_freq=n_eps/10, n_epochs=n_eps)\n",
    "    \n",
    "    probs_test = label_model.predict_proba(L_test)\n",
    "    preds_test = probs_to_preds(probs_test)\n",
    "    f1_test = metric_score(Y_test, preds_test, probs=probs_test, metric='f1')\n",
    "    roc_auc_test = metric_score(Y_test, preds_test, probs=probs_test, metric='roc_auc')\n",
    "\n",
    "    f1_based_test_store[deps_name] = {'f1': f1_test, 'roc_auc': roc_auc_test}\n",
    "    \n",
    "    \n",
    "for deps_name in deps_names:\n",
    "    param = roc_based_store[deps_name]['param']\n",
    "    n_eps = roc_based_store[deps_name]['n_eps']\n",
    "    lr = roc_based_store[deps_name]['lr']\n",
    "    \n",
    "    deps = overall_deps_fn(deps_name, param)\n",
    "    \n",
    "    label_model = Informed_LabelModel(edges = deps, cardinality=2, verbose=True)\n",
    "    label_model.fit(L_train, Y_dev, seed=12345, lr=lr, log_freq=n_eps/10, n_epochs=n_eps)\n",
    "    \n",
    "    probs_test = label_model.predict_proba(L_test)\n",
    "    preds_test = probs_to_preds(probs_test)\n",
    "    f1_test = metric_score(Y_test, preds_test, probs=probs_test, metric='f1')\n",
    "    roc_auc_test = metric_score(Y_test, preds_test, probs=probs_test, metric='roc_auc')\n",
    "\n",
    "    roc_based_test_store[deps_name] = {'f1': f1_test, 'roc_auc': roc_auc_test}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pd.DataFrame(roc_based_store))\n",
    "print(pd.DataFrame(roc_based_test_store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(f1_based_store))\n",
    "print(pd.DataFrame(f1_based_test_store))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun above validation analysis for our methods without very large significance levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from snorkel.analysis import metric_score\n",
    "from snorkel.utils import probs_to_preds\n",
    "\n",
    "epochs_list = [50, 100, 500, 1000, 5000][::-1]\n",
    "sig_list = [0.01, 0.05, 0.1, 0.25]\n",
    "sig_crazy_list = [0.75, 0.9]\n",
    "thresh_list = [0.1, 0.5, 1, 1.5]\n",
    "lr_list = [0.01, 0.025, 0.05, 0.1]\n",
    "\n",
    "deps_names = ['CDGAM'] # !!!!!!!!!! modify this !!!!!!!!!!!\n",
    "\n",
    "info = { 'CDM': {'deps_params': sig_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'NM': {'deps_params': sig_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'NM_NP': {'deps_params': sig_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'CDGAM': {'deps_params': sig_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'Mod_Varma': {'deps_params': thresh_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'Varma': {'deps_params': thresh_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'Varma_Gold': {'deps_params': thresh_list, 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}, \n",
    "            'Empty': {'deps_params': [-1], 'snorkel_eps': epochs_list, 'snorkel_lr':lr_list}}\n",
    "\n",
    "roc_based_store = {key: {'param': -1, 'n_eps': -1, 'lr': -1, 'f1': -1, 'roc_auc': -1} for key in deps_names}\n",
    "\n",
    "f1_based_store = {key: {'param': -1, 'n_eps': -1, 'lr': -1, 'f1': -1, 'roc_auc': -1} for key in deps_names}\n",
    "\n",
    "def overall_deps_fn(deps_name, param):\n",
    "    if deps_name == 'CDM':\n",
    "        deps = CDM(L_dev, Y_dev, k=2, sig=param, policy = 'old', verbose=False, return_more_info = False)\n",
    "    elif deps_name == 'CDGAM':\n",
    "        deps = CDGAM(L_dev, k=2, sig=param, policy = 'old', verbose = False, return_more_info = False)\n",
    "    elif deps_name == 'NM':\n",
    "        deps = NM(L_dev, Y_dev, k=2, sig=param, policy = 'old', verbose=False, return_more_info = False)\n",
    "    elif deps_name == 'NM_NP':\n",
    "        deps = NM(L_dev, Y_dev, k=2, sig=param, policy = 'new', verbose=False, return_more_info = False)\n",
    "    elif deps_name == 'Mod_Varma':\n",
    "        deps = ModVarma_InCov(L_dev, Y_dev, thresh=param)\n",
    "    elif deps_name == 'Varma':\n",
    "        deps = get_varma_edges(L_dev, thresh=param)\n",
    "    elif deps_name == 'Varma_Gold':\n",
    "        deps = get_varma_with_gold_edges(L_dev, Y_dev, thresh=param)\n",
    "    elif deps_name == 'Empty':\n",
    "        deps = []\n",
    "    return deps\n",
    "\n",
    "ct=0\n",
    "total = sum([len(info[deps_name]['deps_params']) for deps_name in deps_names]) * len(epochs_list) * len(lr_list)\n",
    "for deps_name in deps_names:\n",
    "    for param in info[deps_name]['deps_params']:\n",
    "        deps = overall_deps_fn(deps_name, param)\n",
    "        \n",
    "        for n_eps in info[deps_name]['snorkel_eps']:\n",
    "            for lr in info[deps_name]['snorkel_lr']:\n",
    "                \n",
    "                label_model = Informed_LabelModel(edges = deps, cardinality=2, verbose=True)\n",
    "                label_model.fit(L_train, Y_dev, seed=12345, lr=lr, log_freq=n_eps/10, n_epochs=n_eps)\n",
    "                \n",
    "                probs_dev = label_model.predict_proba(L_dev)\n",
    "                preds_dev = probs_to_preds(probs_dev)\n",
    "                f1 = metric_score(Y_dev, preds_dev, probs=probs_dev, metric='f1')\n",
    "                roc_auc = metric_score(Y_dev, preds_dev, probs=probs_dev, metric='roc_auc')\n",
    "                \n",
    "                if roc_auc>roc_based_store[deps_name]['roc_auc']:\n",
    "                    print(deps_name, param, deps, n_eps, lr, \" | roc: \", roc_auc)\n",
    "                    roc_based_store[deps_name] = {'param': param, 'n_eps': n_eps, 'lr': lr, 'f1': f1, 'roc_auc': roc_auc}\n",
    "                \n",
    "                if f1>f1_based_store[deps_name]['f1']:\n",
    "                    print(deps_name, param, deps, n_eps, lr, \" | f1: \", f1)\n",
    "                    f1_based_store[deps_name] = {'param': param, 'n_eps': n_eps, 'lr': lr, 'f1': f1, 'roc_auc': roc_auc}   \n",
    "                \n",
    "                ct +=1\n",
    "                print(ct, \" / \", total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# test using params from validation\n",
    "f1_based_test_store = {key: {'f1': -1, 'roc_auc': -1} for key in deps_names}\n",
    "roc_based_test_store = {key: {'f1': -1, 'roc_auc': -1} for key in deps_names}\n",
    "\n",
    "for deps_name in deps_names:\n",
    "    param = f1_based_store[deps_name]['param']\n",
    "    n_eps = f1_based_store[deps_name]['n_eps']\n",
    "    lr = f1_based_store[deps_name]['lr']\n",
    "    \n",
    "    deps = overall_deps_fn(deps_name, param)\n",
    "    \n",
    "    label_model = Informed_LabelModel(edges = deps, cardinality=2, verbose=True)\n",
    "    label_model.fit(L_train, Y_dev, seed=12345, lr=lr, log_freq=n_eps/10, n_epochs=n_eps)\n",
    "    \n",
    "    probs_test = label_model.predict_proba(L_test)\n",
    "    preds_test = probs_to_preds(probs_test)\n",
    "    f1_test = metric_score(Y_test, preds_test, probs=probs_test, metric='f1')\n",
    "    roc_auc_test = metric_score(Y_test, preds_test, probs=probs_test, metric='roc_auc')\n",
    "\n",
    "    f1_based_test_store[deps_name] = {'f1': f1_test, 'roc_auc': roc_auc_test}\n",
    "    \n",
    "    \n",
    "for deps_name in deps_names:\n",
    "    param = roc_based_store[deps_name]['param']\n",
    "    n_eps = roc_based_store[deps_name]['n_eps']\n",
    "    lr = roc_based_store[deps_name]['lr']\n",
    "    \n",
    "    deps = overall_deps_fn(deps_name, param)\n",
    "    \n",
    "    label_model = Informed_LabelModel(edges = deps, cardinality=2, verbose=True)\n",
    "    label_model.fit(L_train, Y_dev, seed=12345, lr=lr, log_freq=n_eps/10, n_epochs=n_eps)\n",
    "    \n",
    "    probs_test = label_model.predict_proba(L_test)\n",
    "    preds_test = probs_to_preds(probs_test)\n",
    "    f1_test = metric_score(Y_test, preds_test, probs=probs_test, metric='f1')\n",
    "    roc_auc_test = metric_score(Y_test, preds_test, probs=probs_test, metric='roc_auc')\n",
    "\n",
    "    roc_based_test_store[deps_name] = {'f1': f1_test, 'roc_auc': roc_auc_test}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(roc_based_store))\n",
    "print(pd.DataFrame(roc_based_test_store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(f1_based_store))\n",
    "print(pd.DataFrame(f1_based_test_store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
